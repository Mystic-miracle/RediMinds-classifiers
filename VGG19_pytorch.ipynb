{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "233fe963-ed10-4247-9713-c56bed60eaac",
   "metadata": {
    "id": "npmDe_CGKoPO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18fe4430-3847-4090-a9ef-54e3927feda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = 1  # Change this to the index of the desired GPU\n",
    "torch.cuda.set_device(gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cef4425-e758-4c4f-a160-379ec0dd272b",
   "metadata": {
    "id": "rGAyI9xdKxTB"
   },
   "outputs": [],
   "source": [
    "DATADIR = \"RediMinds/Data_Cleaned/\"\n",
    "categories = [\"Anaesthesia_machine\",\"baby_incubator\",\"Bone_saws\",\"C_arm\",\"colonoscope\",\"Curved_Mayo_Scissor\",\"difibrillator\",\"Electrocautery_devices\",\"fetal_doppler\",\"forceps\",\"Heart_Lung_Machine\",\"IABP\",\"IMRT\",\"infusion_pump\",\"Laryngoscopes\",\"mayfield_clamp\",\"Needle_Biopsy_Device\",\"phacoemulsification\",\"Radiofrequency_Ablation_Device\",\"Scalpel\",\"Straight_Dissection_Clamp\",\"Straight_Mayo_Scissor\",\"Suction_Machine\",\"ventilator\",\"x_ray\"]\n",
    "\n",
    "input_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25de4b9e-e088-45c9-a63a-599be090c8e7",
   "metadata": {
    "id": "d0mpzTyLK1l2"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, categories, input_size, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.categories = categories\n",
    "        self.input_size = input_size\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        for cat in self.categories:\n",
    "            path = os.path.join(self.data_dir, cat)\n",
    "            class_num = self.categories.index(cat)\n",
    "            for img_name in os.listdir(path):\n",
    "                try:\n",
    "                    img_path = os.path.join(path, img_name)\n",
    "                    img = Image.open(img_path).convert(\"RGB\")\n",
    "                    img = img.resize(self.input_size)\n",
    "                    if self.transform:\n",
    "                        img = self.transform(img)\n",
    "                    self.data.append(img)\n",
    "                    self.labels.append(class_num)\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8567220c-bfd2-4f46-b4aa-65d1c7c7cfd9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4y-YObECLeC2",
    "outputId": "37531a35-eaa5-45a9-a0d6-b7a5e3d683ad"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.RandomResizedCrop(input_size, scale=(0.8, 1.0)),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomRotation(15),\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast = 0.25, saturation = 0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.2, 0.2, 0.2))\n",
    "])\n",
    "\n",
    "dataset = CustomDataset(DATADIR, categories, input_size, transform=transform)\n",
    "\n",
    "train_size = int(0.70 * len(dataset))\n",
    "validation_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - validation_size\n",
    "train_dataset, validation_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size,validation_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b45cdf38-c35e-421f-b2df-839aa7406faf",
   "metadata": {
    "id": "HDjJkmtgLjVZ"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "370fc8a6-44a4-44b2-a44d-c8510b7b4376",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVGG19(nn.Module):\n",
    "    def __init__(self, num_classes, fine_tune_layers=3):\n",
    "        super(CustomVGG19, self).__init__()\n",
    "        vgg19 = models.vgg19(pretrained='imagenet')\n",
    "\n",
    "        # Freeze all layers by default\n",
    "        for param in vgg19.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Make the last block trainable\n",
    "        for layer in vgg19.features[-fine_tune_layers:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        for param in vgg19.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.vgg19 = vgg19\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(4096,2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            # #nn.Linear(512, num_classes)\n",
    "        )\n",
    "        self.cnn1 = nn.Conv2d(2048, 2048, kernel_size = 3, padding = 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.cnn2 = nn.Conv2d(2048, 512, kernel_size = 3, padding = 1)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.cnn3 = nn.Conv2d(512,64, kernel_size = 3, padding = 1)\n",
    "        self.cnnl = nn.Conv2d(64, num_classes, kernel_size=3,padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vgg19.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.head(x)\n",
    "        x = x.view(x.size(0), -1,1,1)\n",
    "        x = self.cnn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.cnn3(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.cnnl(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34f11ff3-282c-41e0-b72b-d2b448dc2ca6",
   "metadata": {
    "id": "x03pIT-qMgF1"
   },
   "outputs": [],
   "source": [
    "num_classes = len(categories)\n",
    "model = CustomVGG19(num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b5c38d1-624a-4c01-8a34-79838571dc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg19.features.0.weight: requires_grad=False\n",
      "vgg19.features.0.bias: requires_grad=False\n",
      "vgg19.features.2.weight: requires_grad=False\n",
      "vgg19.features.2.bias: requires_grad=False\n",
      "vgg19.features.5.weight: requires_grad=False\n",
      "vgg19.features.5.bias: requires_grad=False\n",
      "vgg19.features.7.weight: requires_grad=False\n",
      "vgg19.features.7.bias: requires_grad=False\n",
      "vgg19.features.10.weight: requires_grad=False\n",
      "vgg19.features.10.bias: requires_grad=False\n",
      "vgg19.features.12.weight: requires_grad=False\n",
      "vgg19.features.12.bias: requires_grad=False\n",
      "vgg19.features.14.weight: requires_grad=False\n",
      "vgg19.features.14.bias: requires_grad=False\n",
      "vgg19.features.16.weight: requires_grad=False\n",
      "vgg19.features.16.bias: requires_grad=False\n",
      "vgg19.features.19.weight: requires_grad=False\n",
      "vgg19.features.19.bias: requires_grad=False\n",
      "vgg19.features.21.weight: requires_grad=False\n",
      "vgg19.features.21.bias: requires_grad=False\n",
      "vgg19.features.23.weight: requires_grad=False\n",
      "vgg19.features.23.bias: requires_grad=False\n",
      "vgg19.features.25.weight: requires_grad=False\n",
      "vgg19.features.25.bias: requires_grad=False\n",
      "vgg19.features.28.weight: requires_grad=False\n",
      "vgg19.features.28.bias: requires_grad=False\n",
      "vgg19.features.30.weight: requires_grad=False\n",
      "vgg19.features.30.bias: requires_grad=False\n",
      "vgg19.features.32.weight: requires_grad=False\n",
      "vgg19.features.32.bias: requires_grad=False\n",
      "vgg19.features.34.weight: requires_grad=True\n",
      "vgg19.features.34.bias: requires_grad=True\n",
      "vgg19.classifier.0.weight: requires_grad=True\n",
      "vgg19.classifier.0.bias: requires_grad=True\n",
      "vgg19.classifier.3.weight: requires_grad=True\n",
      "vgg19.classifier.3.bias: requires_grad=True\n",
      "vgg19.classifier.6.weight: requires_grad=True\n",
      "vgg19.classifier.6.bias: requires_grad=True\n",
      "head.0.weight: requires_grad=True\n",
      "head.0.bias: requires_grad=True\n",
      "head.3.weight: requires_grad=True\n",
      "head.3.bias: requires_grad=True\n",
      "head.6.weight: requires_grad=True\n",
      "head.6.bias: requires_grad=True\n",
      "cnn1.weight: requires_grad=True\n",
      "cnn1.bias: requires_grad=True\n",
      "cnn2.weight: requires_grad=True\n",
      "cnn2.bias: requires_grad=True\n",
      "cnn3.weight: requires_grad=True\n",
      "cnn3.bias: requires_grad=True\n",
      "cnnl.weight: requires_grad=True\n",
      "cnnl.bias: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d21def0-fa42-478a-b67c-a6b897a593c2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0eNmu2UJMg7v",
    "outputId": "26814188-ecd3-4f26-bf3f-a7578768454f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 2.8226 | Train Acc: 0.0833 | Val Loss: 2.6903 | Val Acc: 0.1478\n",
      "Epoch 2/50 | Train Loss: 2.3249 | Train Acc: 0.1708 | Val Loss: 2.1505 | Val Acc: 0.2138\n",
      "Epoch 3/50 | Train Loss: 1.9815 | Train Acc: 0.2375 | Val Loss: 1.9816 | Val Acc: 0.2310\n",
      "Epoch 4/50 | Train Loss: 1.6979 | Train Acc: 0.3309 | Val Loss: 1.8859 | Val Acc: 0.3343\n",
      "Epoch 5/50 | Train Loss: 1.5489 | Train Acc: 0.3896 | Val Loss: 1.6063 | Val Acc: 0.4247\n",
      "Epoch 6/50 | Train Loss: 1.3660 | Train Acc: 0.4562 | Val Loss: 1.4947 | Val Acc: 0.4304\n",
      "Epoch 7/50 | Train Loss: 1.2022 | Train Acc: 0.5072 | Val Loss: 1.4396 | Val Acc: 0.4706\n",
      "Epoch 8/50 | Train Loss: 1.1151 | Train Acc: 0.5813 | Val Loss: 1.6311 | Val Acc: 0.5323\n",
      "Epoch 9/50 | Train Loss: 1.0601 | Train Acc: 0.6018 | Val Loss: 1.6918 | Val Acc: 0.5265\n",
      "Epoch 10/50 | Train Loss: 1.0043 | Train Acc: 0.6212 | Val Loss: 1.6528 | Val Acc: 0.4749\n",
      "Epoch 11/50 | Train Loss: 1.1004 | Train Acc: 0.6147 | Val Loss: 1.8069 | Val Acc: 0.5194\n",
      "Epoch 12/50 | Train Loss: 1.1174 | Train Acc: 0.6212 | Val Loss: 1.9428 | Val Acc: 0.5395\n",
      "Epoch 13/50 | Train Loss: 1.0035 | Train Acc: 0.6418 | Val Loss: 1.4911 | Val Acc: 0.5768\n",
      "Epoch 14/50 | Train Loss: 0.8595 | Train Acc: 0.6876 | Val Loss: 2.0784 | Val Acc: 0.5581\n",
      "Epoch 15/50 | Train Loss: 1.2111 | Train Acc: 0.6780 | Val Loss: 1.9842 | Val Acc: 0.5782\n",
      "Epoch 16/50 | Train Loss: 0.7426 | Train Acc: 0.7204 | Val Loss: 1.5463 | Val Acc: 0.6112\n",
      "Epoch 17/50 | Train Loss: 0.6806 | Train Acc: 0.7432 | Val Loss: 1.9402 | Val Acc: 0.6112\n",
      "Epoch 18/50 | Train Loss: 0.7848 | Train Acc: 0.7161 | Val Loss: 1.9115 | Val Acc: 0.6126\n",
      "Epoch 19/50 | Train Loss: 0.6444 | Train Acc: 0.7465 | Val Loss: 1.9413 | Val Acc: 0.6485\n",
      "Epoch 20/50 | Train Loss: 0.6508 | Train Acc: 0.7558 | Val Loss: 2.0958 | Val Acc: 0.6399\n",
      "Epoch 21/50 | Train Loss: 0.6311 | Train Acc: 0.7588 | Val Loss: 1.9669 | Val Acc: 0.6327\n",
      "Epoch 22/50 | Train Loss: 0.8405 | Train Acc: 0.7478 | Val Loss: 1.4609 | Val Acc: 0.6212\n",
      "Epoch 23/50 | Train Loss: 0.7382 | Train Acc: 0.7561 | Val Loss: 2.2454 | Val Acc: 0.6327\n",
      "Epoch 24/50 | Train Loss: 0.5631 | Train Acc: 0.7819 | Val Loss: 2.9248 | Val Acc: 0.6184\n",
      "Epoch 25/50 | Train Loss: 0.6686 | Train Acc: 0.7659 | Val Loss: 2.0957 | Val Acc: 0.6671\n",
      "Epoch 26/50 | Train Loss: 0.6610 | Train Acc: 0.7800 | Val Loss: 1.9638 | Val Acc: 0.6298\n",
      "Epoch 27/50 | Train Loss: 0.5581 | Train Acc: 0.7988 | Val Loss: 2.6044 | Val Acc: 0.6528\n",
      "Epoch 28/50 | Train Loss: 0.5555 | Train Acc: 0.7929 | Val Loss: 1.7726 | Val Acc: 0.6141\n",
      "Epoch 29/50 | Train Loss: 0.7084 | Train Acc: 0.7899 | Val Loss: 2.5474 | Val Acc: 0.6700\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 22\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     23\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m train_corrects \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_acc_history = []\n",
    "train_loss_history = []\n",
    "val_acc_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss, train_corrects = 0.0, 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        labels = torch.squeeze(labels)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_corrects += torch.sum(preds == labels).item()\n",
    "\n",
    "    train_loss = train_loss / len(train_dataset)\n",
    "    train_acc = train_corrects / len(train_dataset)\n",
    "    train_acc_history.append(train_acc)\n",
    "    train_loss_history.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss, val_corrects = 0.0, 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_corrects += torch.sum(preds == labels).item()\n",
    "\n",
    "    val_loss = val_loss / len(validation_dataset)\n",
    "    val_acc = val_corrects / len(validation_dataset)\n",
    "\n",
    "    val_acc_history.append(val_acc)\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940e07ab-98be-4c0e-9c8c-4a5be3401315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, num_epochs+1), train_acc_history, label='Train')\n",
    "plt.plot(range(1, num_epochs+1), val_acc_history, label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(range(1, num_epochs+1), train_loss_history, label='Train')\n",
    "plt.plot(range(1, num_epochs+1), val_loss_history, label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7197d38-2723-43af-88d4-1d508df12047",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss, test_corrects = 0.0, 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        test_corrects += torch.sum(preds == labels).item()\n",
    "\n",
    "test_loss = test_loss / len(test_dataset)\n",
    "test_acc = test_corrects / len(test_dataset)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd8c2a-a8c3-40ad-acea-9e6cdcf61340",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
